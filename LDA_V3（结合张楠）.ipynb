{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os # 系统库，用于处理文件和目录\n",
    "import re # 正则表达式库，用于对文本进行清洗\n",
    "# 中文分词库，用于对中文文本进行分词处理\n",
    "import jieba\n",
    "# 数据处理和分析库，用于处理和分析数据\n",
    "import pandas as pd\n",
    "# Gensim库中的corpora和models模块，用于主题建模和文本处理\n",
    "from gensim import corpora, models\n",
    "# Scikit-learn库中的CountVectorizer，用于将文本转换为词频矩阵\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# 绘图库，用于数据可视化\n",
    "import matplotlib.pyplot as plt\n",
    "# 忽略警告信息\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=Warning)\n",
    "# 数值计算库，用于数值运算\n",
    "import numpy as np\n",
    "# pyLDAvis库中的gensim_models模块，用于可视化LDA主题模型\n",
    "import pyLDAvis.gensim_models \n",
    "# Gensim库中的CoherenceModel、LdaModel和Dictionary，用于计算主题一致性和LDA模型\n",
    "from gensim.models.coherencemodel import CoherenceModel\n",
    "from gensim.models.ldamodel import LdaModel\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "# 中文显示支持，设置字体为SimHei\n",
    "plt.rcParams['font.sans-serif'] = ['SimHei']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 新词文件的路径\n",
    "newwords_list = 'newwords.txt'  # 替换为你的文件路径\n",
    "\n",
    "# 从文件中读取新词并添加到jieba字典中\n",
    "with open(newwords_list, 'r', encoding='utf-8') as file:\n",
    "    for word in file:\n",
    "        jieba.add_word(word.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取停用词文件，返回停用词列\n",
    "def remove_stopwords():\n",
    "    stop_word = [line.strip() for line in open('stopwords.txt', 'r', encoding='utf-8')]\n",
    "    return stop_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分词统计函数，对文本进行分词并统计词频\n",
    "def tongji(text, stopwords):\n",
    "    text = text.replace('\\n', '').replace(' ', '').replace('\\r', '').replace('\\u3000', '')\\\n",
    "        .replace('\\t', '').replace('\\xa0', '').replace('\\u2002', '').replace('\\ufeff', '')\n",
    "    words = jieba.cut(text)\n",
    "    words = [str(w) for w in words if str(w) not in stopwords]\n",
    "    for word in words:\n",
    "        if word not in words_dic:\n",
    "            words_dic[word] = 0\n",
    "        words_dic[word] += 1\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 文本分词函数，对文本进行分词并返回连接后的字符串\n",
    "def tokenize(text, stopwords):\n",
    "    text = text.replace('\\n', '').replace(' ', '').replace('\\r', '').replace('\\u3000', '')\\\n",
    "        .replace('\\t', '').replace('\\xa0', '').replace('\\u2002', '').replace('\\ufeff', '')\n",
    "    words = jieba.cut(text)\n",
    "    words = [str(w) for w in words if str(w) not in stopwords]\n",
    "    return ' '.join(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 数据预处理函数，读取数据、处理时间列、分词、去除低频和高频词，并返回处理后的数据框\n",
    "def data_process(limit_min=1, limit_max=100):\n",
    "    stopwords = remove_stopwords()\n",
    "    df = pd.read_txt(\"【文本】政府数据治理机构职能\")\n",
    "\n",
    "    # 应用tongji函数，统计词频\n",
    "    df['统计'] = df['全文'].apply(tongji, args=(stopwords,))\n",
    "    print(len(words_dic))\n",
    "    \n",
    "    # 剔除低频和高频词\n",
    "    words_rm = [k for k, v in words_dic.items() if v <= limit_min or v >= limit_max]\n",
    "    print(len(words_rm))\n",
    "    del df[\"统计\"]\n",
    "    stopwords = stopwords + words_rm\n",
    "    stopwords = set(stopwords)\n",
    "    print(len(stopwords))\n",
    "    \n",
    "    # 应用tokenize函数，进行分词\n",
    "    df['content'] = df['全文'].apply(tokenize, args=(stopwords,))\n",
    "    \n",
    "    # 删除缺失值\n",
    "    df.dropna(subset=[\"content\"], inplace=True)\n",
    "    \n",
    "    # 打印处理后的数据框信息\n",
    "    documents = df['content'].tolist()\n",
    "    documents = [i.split(\" \") for i in documents]\n",
    "    print(max(len(sublist) for sublist in documents))\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pandas' has no attribute 'read_txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 读取数据\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m df \u001b[38;5;241m=\u001b[39m data_process(limit_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m,limit_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m)\n\u001b[0;32m      3\u001b[0m df\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./res/数据清洗.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m df\n",
      "Cell \u001b[1;32mIn[22], line 4\u001b[0m, in \u001b[0;36mdata_process\u001b[1;34m(limit_min, limit_max)\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdata_process\u001b[39m(limit_min\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, limit_max\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m):\n\u001b[0;32m      3\u001b[0m     stopwords \u001b[38;5;241m=\u001b[39m remove_stopwords()\n\u001b[1;32m----> 4\u001b[0m     df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_txt(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m【文本】政府数据治理机构职能\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m# 应用tongji函数，统计词频\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m统计\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m全文\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(tongji, args\u001b[38;5;241m=\u001b[39m(stopwords,))\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'pandas' has no attribute 'read_txt'"
     ]
    }
   ],
   "source": [
    "# 读取数据\n",
    "df = data_process(limit_min=5,limit_max=50)\n",
    "df.to_excel(\"./res/数据清洗.xlsx\",index=False)\n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'clean_stopword' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m text \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      6\u001b[0m text \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(re\u001b[38;5;241m.\u001b[39mfindall(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m[\u001b[39m\u001b[38;5;130;01m\\u4e00\u001b[39;00m\u001b[38;5;124m-\u001b[39m\u001b[38;5;130;01m\\u9fa5\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m'\u001b[39m, text)) \u001b[38;5;66;03m# 只保留中文\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m doc \u001b[38;5;241m=\u001b[39m clean_stopword(text, stopwords)\n\u001b[0;32m      8\u001b[0m data\u001b[38;5;241m.\u001b[39mappend(doc)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'clean_stopword' is not defined"
     ]
    }
   ],
   "source": [
    "data = []\n",
    "file_paths = os.listdir('【文本】政府数据治理机构职能')\n",
    "for i, file in enumerate(file_paths):\n",
    "    with open('【文本】政府数据治理机构职能/'+file) as f:\n",
    "        text = f.read()\n",
    "        text = ''.join(re.findall('[\\u4e00-\\u9fa5]', text)) # 只保留中文\n",
    "        doc = clean_stopword(text, stopwords)\n",
    "        data.append(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
